{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolez9911/colab/blob/main/AdvML_L4S1N1_Continous_Learning_Adaline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kmErm81JxJf"
      },
      "source": [
        "# Continuous Learning\n",
        "\n",
        "\n",
        "The focus of this notebook is the implementation of the continuous learning approaches introduced in the lecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpjm17VJJxJj"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "qkwSH8xQJxJk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmH1DX05JxJm"
      },
      "source": [
        "## MNIST Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "_7otUFG_JxJm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sorts the dataset by target (i.e the numbers 0-9). Sorts only the first 60000 entries (the training set).\n",
        "def sort_by_target(mnist):\n",
        "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
        "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
        "    mnist.data[:60000] = mnist.data[reorder_train]\n",
        "    mnist.target[:60000] = mnist.target[reorder_train]\n",
        "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
        "    mnist.target[60000:] = mnist.target[reorder_test + 60000]\n",
        "\n",
        "\n",
        "# Download and sort the dataset\n",
        "try:\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
        "    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
        "except ImportError:\n",
        "    from sklearn.datasets import fetch_mldata\n",
        "    mnist = fetch_mldata('MNIST original')\n",
        "mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "\n",
        "print(mnist.target.shape)\n",
        "print(mnist.data.shape)\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "# Select 5000 values for 0 and 9 each by manually checking the boundaries or by checking for target value.\n",
        "mnist_4_target = mnist.target[0:5000]\n",
        "mnist_4_data = mnist.data[0:5000]\n",
        "mnist_5_target = mnist.target[5000:10000]\n",
        "mnist_5_data = mnist.data[5000:10000]\n",
        "#mnist_4_target = mnist.target[25000:30000]\n",
        "#mnist_4_data = mnist.data[25000:30000]\n",
        "#mnist_5_target = mnist.target[31000:36000]\n",
        "#mnist_5_data = mnist.data[31000:36000]\n",
        "mnist_bin_4_5_data = np.concatenate((mnist_4_data, mnist_5_data))\n",
        "mnist_bin_4_5_target = np.concatenate((mnist_4_target, mnist_5_target))\n",
        "mnist_bin_4_5_target = np.where(mnist_bin_4_5_target == 0, -1, 1)\n",
        "\n",
        "# Shuffle should be something that the students have to identify as an element that impacts the learning\n",
        "X, y = shuffle(mnist_bin_4_5_data, mnist_bin_4_5_target)\n",
        "len(mnist_bin_4_5_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "eTVmgh3UJxJn"
      },
      "source": [
        "## Adaline with Gradient Descent\n",
        "\n",
        "It is an implementation of Gradient Descent with Batch Gradient Descent calculation.\n",
        "\n",
        "Note: This implementation is taken from Raschka et al. Machine Learning with Pyhon and seems to\n",
        "contain a bug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "9GNRcBbjJxJn"
      },
      "outputs": [],
      "source": [
        "class AdalineGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value in each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "        self.w_ = np.float64(self.w_)\n",
        "        self.cost_ = []\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            net_input = self.net_input(X)\n",
        "            # Please note that the \"activation\" method has no effect\n",
        "            # in the code since it is simply an identity function. We\n",
        "            # could write `output = self.net_input(X)` directly instead.\n",
        "            # The purpose of the activation is more conceptual, i.e.,\n",
        "            # in the case of logistic regression (as we will see later),\n",
        "            # we could change it to\n",
        "            # a sigmoid function to implement a logistic regression classifier.\n",
        "            output = self.activation(net_input)\n",
        "            errors = (y - output)\n",
        "            self.w_[1:] -= self.eta * (X.T.dot(errors))/X.shape[0]\n",
        "            self.w_[0] += self.eta * errors.sum()\n",
        "            cost = ((errors**2).sum() / 2.0)/X.shape[0]\n",
        "            self.cost_.append(cost)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EnfIby-JxJp"
      },
      "source": [
        "## Plotting the Cost\n",
        "\n",
        "The next cell plots the cost of the Adaline implementation in order to demonstrate the sensitivity\n",
        "of the gradient descent with regard to the step size.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "vez3UCwrJxJq"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "num_epochs = 100\n",
        "ada1 = AdalineGD(n_iter=num_epochs, eta=0.000001).fit(X_image_normalized, y)\n",
        "ax[0].plot(range(1, len(ada1.cost_) + 1), ada1.cost_, marker='o')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('log(Sum-squared-error)')\n",
        "ax[0].set_title('Adaline - Learning rate 0.01')\n",
        "\n",
        "ada2 = AdalineGD(n_iter=num_epochs, eta=0.000005).fit(X_image_normalized, y)\n",
        "ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Sum-squared-error')\n",
        "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
        "\n",
        "# plt.savefig('images/02_11.png', dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaF1gxuQJxJr"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "Stochastic Gradient Descent instead of Batch Gradient Descent calculates the updates after each sample.\n",
        "It then shuffles the samples before calculating with the next sample.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "sLgotpX7JxJr"
      },
      "outputs": [],
      "source": [
        "class AdalineSGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    shuffle : bool (default: True)\n",
        "      Shuffles training data every epoch if True to prevent cycles.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value averaged over all\n",
        "      training samples in each epoch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.w_initialized = False\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_samples, n_features]\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_samples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.cost_ = []\n",
        "        for i in range(self.n_iter):\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "            cost = []\n",
        "            for xi, target in zip(X, y):\n",
        "                cost.append(self._update_weights(xi, target))\n",
        "            avg_cost = sum(cost) / len(y)\n",
        "            self.cost_.append(avg_cost)\n",
        "        return self\n",
        "\n",
        "    def partial_fit(self, X, y):\n",
        "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
        "        if not self.w_initialized:\n",
        "            self._initialize_weights(X.shape[1])\n",
        "        if y.ravel().shape[0] > 1:\n",
        "            for xi, target in zip(X, y):\n",
        "                self._update_weights(xi, target)\n",
        "        else:\n",
        "            self._update_weights(X, y)\n",
        "        return self\n",
        "\n",
        "    def _shuffle(self, X, y):\n",
        "        \"\"\"Shuffle training data\"\"\"\n",
        "        r = self.rgen.permutation(len(y))\n",
        "        return X[r], y[r]\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
        "        self.w_initialized = True\n",
        "\n",
        "    def _update_weights(self, xi, target):\n",
        "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
        "        output = self.activation(self.net_input(xi))\n",
        "        error = (target - output)\n",
        "        self.w_[1:] += self.eta * xi.dot(error)\n",
        "        self.w_[0] += self.eta * error\n",
        "        cost = 0.5 * error**2\n",
        "        return cost\n",
        "\n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        },
        "id": "Ozn1r4WYJxJs"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Normalisation of the features is a commonly applied technique to optimize the learning of\n",
        "the gradient descent if approach:\n",
        "\n",
        "1. normalize the Feature Matrix using the cell below\n",
        "2. analyze the effect of normalize on the feature matrix\n",
        "3. train AdalineSGD with a normalized and non-normalized feature matrix.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "kRWNfmh9JxJs"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "# standardize features\n",
        "\n",
        "X_normed = normalize(X, axis=0, norm='max')\n",
        "X_image_normalized = X*(1.0/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "sjdESVLtJxJt"
      },
      "outputs": [],
      "source": [
        "ada = AdalineGD(n_iter=100, eta=0.000001)\n",
        "ada.fit(X_normed, y)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Sum-squared-error')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_14_2.png', dpi=300)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "zgVtlfe4JxJt"
      },
      "outputs": [],
      "source": [
        "ada = AdalineSGD(n_iter=100, eta=0.001, random_state=1)\n",
        "ada.fit(X_normed, y)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_1.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Sum-squared-error')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_2.png', dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "PyCharm (Perceptron)",
      "language": "python",
      "name": "pycharm-d231b7f0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}