{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicolez9911/colab/blob/main/AdvML_L6S1_N1_Random_Forests_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFZpHRABT2VL"
      },
      "source": [
        "# Decision Trees Exercises\n",
        "\n",
        "This notebook focuses on decision tree exercises.\n",
        "Two main exercises are in focus:\n",
        "\n",
        "1. Exploring the relationship between:\n",
        "   * Tree depth\n",
        "   * Training & Test Performance\n",
        "\n",
        "2. Applying Discretization (Binning) To The Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1GTZlw5T2VS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "# add library module to PYTHONPATH\n",
        "sys.path.append(f\"{os.getcwd()}/..\")\n",
        "\n",
        "import sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from dtreeviz.trees import *\n",
        "\n",
        "import graphviz\n",
        "import pandas as pd\n",
        "\n",
        "random_state = 1234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtSUC0ZBT2VX"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2 # Reload all modules (except those excluded by %aimport) every time before executing code.\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLbsWO82T2Vm"
      },
      "source": [
        "# Load data\n",
        "\n",
        "We will use the Titanic dataset as a basis for testing Decision Trees.\n",
        "\n",
        "The `Titanic` dataset consists of two elements:\n",
        "* Original passenger data\n",
        "* Survival as target variabel\n",
        "\n",
        "The passenger data contains the following columns:\n",
        "\n",
        "* pclass: A proxy for socio-economic status (SES)\n",
        "   * 1st = Upper\n",
        "   * 2nd = Middle\n",
        "   * 3rd = Lower\n",
        "\n",
        "* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
        "\n",
        "* sibsp: The dataset defines family relations in this way...\n",
        "    * Sibling = brother, sister, stepbrother, stepsister\n",
        "    * Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
        "\n",
        "* parch: The dataset defines family relations in this way...\n",
        "    * Parent = mother, father\n",
        "    * Child = daughter, son, stepdaughter, stepson\n",
        "    * Some children travelled only with a nanny, therefore parch=0 for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lryUTnWeT2Vo"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv(\"./titanic_dataset/titanic.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WegJ4OsHT2WC"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Before we can start working with the dataframe we have to fix the missing values.\n",
        "* fillna() allows us to do inplace replacement of those values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w93xeYpT2WE"
      },
      "outputs": [],
      "source": [
        "# Fill missing values for Age\n",
        "dataframe.fillna({\"Age\":dataframe.Age.mean()}, inplace=True)\n",
        "# Encode categorical variables\n",
        "# The `astype(\"category\").cat.codes` call encodes a categorical label in numerical form\n",
        "dataframe[\"Sex_label\"] = dataframe.Sex.astype(\"category\").cat.codes\n",
        "dataframe[\"Cabin_label\"] = dataframe.Cabin.astype(\"category\").cat.codes\n",
        "dataframe[\"Embarked_label\"] = dataframe.Embarked.astype(\"category\").cat.codes\n",
        "\n",
        "features = [\"Pclass\", \"Age\", \"Fare\", \"Sex_label\", \"Cabin_label\", \"Embarked_label\"]\n",
        "target = \"Survived\"\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataframe[features], dataframe[target], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPKNS3H7T2WP"
      },
      "source": [
        "## Model training\n",
        "We will train with full data, the goal is to just interpretate the tree structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVi1DwWtT2WQ",
        "outputId": "ae4e7f6f-a8ef-4896-867f-610e0b1b862b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=1234, splitter='best')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtc = DecisionTreeClassifier(max_depth=5, random_state=random_state)\n",
        "dtc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xkl4xJiT2WT"
      },
      "source": [
        "### Exercise 1: Impact of Depth on Test and Train Performance\n",
        "\n",
        "Evaluate the performance for the decision tree based on the train & test sets defined in the cells above:\n",
        "\n",
        "* X_train, y_train\n",
        "* X_test, y_test\n",
        "\n",
        "Focus your evaluation on the hyper-parameter `max_depth` in the range of `2` to `20` and observe the impact on the accuracy on the train and test level.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufqIWVAHgsiv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAExYVMBgsiv"
      },
      "source": [
        "### Exercise 2: Plotting Test and Train over Depth\n",
        "\n",
        "Plot train and test accuracy over the hyper-parameter `max_depth` in the range of `2` to `20` in a single plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiDgLPC4gsiv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRhksxXT2Wj"
      },
      "source": [
        "### Exercise 3: Model interpretation\n",
        "\n",
        "Use the visualisation and interpretation capability below to analyse trees at the extreme edges of the hyperparameter depth for values:\n",
        "\n",
        "* 2\n",
        "* 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXfEzO3kT2Wj"
      },
      "outputs": [],
      "source": [
        "class_names = list(dtc.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MBf0UpT2Wl"
      },
      "outputs": [],
      "source": [
        "dtreeviz(dtc, X_train, y_train, features, target, class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIIZ927tT2Ws"
      },
      "outputs": [],
      "source": [
        "# fancy=False\n",
        "dtreeviz(dtc, X_train, y_train, features, target, class_names, fancy=False )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQAlgQJLT2Wu"
      },
      "source": [
        "### Leaf samples\n",
        "Each node contains some important details. One of these is 'samples', which shows the number of samples from training set which pass through that node.<br>\n",
        "Would be very helpful to see the number of samples from each leaf. Why? Because it shows the confidence of leaf prediction. <br>\n",
        "For example, if we have a leaf with good prediction(ex. gini=0.0) but very few samples in in (ex. samples=1), this could be the sign of overfiting. If our leaf would contains more samples, then we could be more confident about its prediction. <br>\n",
        "\n",
        "This is how we can easily get leaf samples from a big tree structure (using plots or plain text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gdFyuK1T2Wu"
      },
      "outputs": [],
      "source": [
        "viz_leaf_samples(dtc, figsize=(20,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0I-xDaAT2Wx"
      },
      "outputs": [],
      "source": [
        "ctreeviz_leaf_samples(dtc, display_type=\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkYNfb4iT2W1"
      },
      "outputs": [],
      "source": [
        "#Useful when you want to easily see the general distribution of leaf samples.\n",
        "viz_leaf_samples(dtc, display_type=\"hist\", bins=30, figsize=(20,7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlGBxP3DT2W2"
      },
      "outputs": [],
      "source": [
        "viz_leaf_samples(dtc, display_type=\"hist\", bins=30, figsize=(20,7), min_samples=3, max_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9BWIUDT2W6"
      },
      "source": [
        "### Leaf samples by class\n",
        "Here we can see the number of samples from each leaf by its classes. <br>\n",
        "The leaf with id 78 contains a lot of samples from training set and mojority of them from class 0. In leaf 17 all samples are from class 1. Would be very helpful to see how the samples from these leaves look, what do they have in common. This is a way to get domain knowledge about our dataset using a ML driven approach. <br>\n",
        "More about how we can get the training samples from a leaf in the near future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVysEhyIT2W7"
      },
      "outputs": [],
      "source": [
        "ctreeviz_leaf_samples(dtc, figsize=(20,7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOU8YiLPgsix"
      },
      "source": [
        "### Discretization & Binning\n",
        "\n",
        "Discretization is the process of turning a continuous variable or representation into an integer (natural number) representation.\n",
        "Binning is the process of reducing the range of possible values by introducing a set of bins.\n",
        "\n",
        "Discretize:\n",
        "[0.1, 0.4, 1.2, 7.8] -> [0, 0, 1, 8]\n",
        "\n",
        "Binning: With 4 bins [0,2.5]::1,[2.5,5]::2,[5,7.5]::3,[7.5,10]::4\n",
        "\n",
        "[0.1, 0.4, 1.2, 7.8] -> [1, 1, 1, 4]\n",
        "\n",
        "Discretization and Binning can have positive effect on ML performance by reducing the feature space.\n",
        "\n",
        "* Less possible feature values means more observations\n",
        "* More observations means more opportunities to set a helpful weight for a feature observation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2iiXJUWgsix"
      },
      "source": [
        "### Exercise: Binning\n",
        "\n",
        "Take a look at the input data for the titanic dataset.\n",
        "\n",
        "* Identify a feature that is suited for binning\n",
        "* Apply binning to the feature and re-train the decision tree\n",
        "* Try different bins and observe the effect on train and test accuracy and the interpretability of the model.\n",
        "\n",
        "The functionality of the `loc` function should be helpful when it comes to transforming the original values into binned values.\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html\n",
        "\n",
        "This allows in place replacement as shown below:\n",
        "\n",
        "```\n",
        "dataset.loc[(dataset['FeatureName'] > 24.91) & (dataset['FeatureName'] <= 40), 'FeatureName'] = 3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9kH1-L_gsix"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}